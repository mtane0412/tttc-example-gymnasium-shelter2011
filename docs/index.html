<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="./_next/static/css/d1386df84e9d164b.css" as="style" crossorigin=""/><link rel="stylesheet" href="./_next/static/css/d1386df84e9d164b.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="./_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="./_next/static/chunks/webpack-1e7d36e3ced660e3.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/main-75623049b75f64cc.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/pages/_app-41fcb505e2a6ec3b.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/3e199aef-8a882f67d84a9743.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/413-fe7ad88e9897fb6d.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/664-4ccc24e47970c0fa.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/550-a21c3768867c648a.js" defer="" crossorigin=""></script><script src="./_next/static/chunks/pages/index-cc72100c2afe033d.js" defer="" crossorigin=""></script><script src="./_next/static/JWsllBIcgSGhKLoictODg/_buildManifest.js" defer="" crossorigin=""></script><script src="./_next/static/JWsllBIcgSGhKLoictODg/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"result":{"clusters":[{"cluster":"記事の持続的な価値","cluster_id":"1","takeaways":"参加者は、過去の情報や経験談が現在でも重要であり、語り継がれるべきだと強調しています。特に、実務的な経験談やマニュアルとしての価値が高い情報は、再度確認し、保存する価値があると述べています。","arguments":[{"arg_id":"A1_0","argument":"これは、今から気にしないといけないことか？","comment_id":"1","x":10.296356,"y":18.30583,"p":0.7835684212861425},{"arg_id":"A2_0","argument":"この内容は数年前にツイッターで読んだが、今も教訓として色あせない。語り継がれて欲しい。","comment_id":"2","x":10.830266,"y":18.801115,"p":1},{"arg_id":"A4_0","argument":"1年前の記事だが、貴重で実務的な経験談は多くの人に読まれるべきである。","comment_id":"4","x":10.372056,"y":19.155092,"p":1},{"arg_id":"A6_0","argument":"2015年6月25日","comment_id":"6","x":9.81118,"y":18.945557,"p":0.9915452850701962},{"arg_id":"A7_0","argument":"読んだことを忘れているので、再度ブックマークする。","comment_id":"7","x":11.274783,"y":19.22904,"p":0.7954400570541246},{"arg_id":"A11_0","argument":"これはマニュアルとしても通用する永久保存版だと思う。","comment_id":"11","x":11.525676,"y":18.495037,"p":0.8120963009491853},{"arg_id":"A12_0","argument":"非常に参考になる体験記。","comment_id":"12","x":11.044566,"y":18.06035,"p":0.8001419253325383}]},{"cluster":"震災体験談の共有","cluster_id":"0","takeaways":"東日本大震災では、東日本全体で21,000人が命を失い、または行方不明となりました。被災者の一人である佐藤一男氏は、岩手県陸前高田市米崎小学校の体育館で二ヶ月間避難所生活を送り、その運営に携わりました。彼の経験は、防災対策の重要性と避難所運営の課題を浮き彫りにしています。","arguments":[{"arg_id":"A3_0","argument":"防災","comment_id":"3","x":10.027298,"y":20.736618,"p":0.99511544766781},{"arg_id":"A5_0","argument":"東日本大震災では、東日本全体で21,000人もの人が命を失い、または行方不明となりました。私も被災し、岩手県陸前高田市米崎小学校の体育館で二ヶ月間避難所生活をしながら避難所運営を経験しました。","comment_id":"5","x":10.562809,"y":20.109604,"p":1},{"arg_id":"A9_1","argument":"[東日本大震災][防災][地震] / “東日本大震災、体育館避難所で起きたこと / 佐藤一男 / 防災士 | SYNODOS …”","comment_id":"9","x":10.628125,"y":20.794563,"p":1}]},{"cluster":"行動喚起","cluster_id":"2","takeaways":"この記事を読むことで、震災に対する共通意識が形成されやすくなります。地震が発生した際に「何か力になりたい」と考える人々にとって、この記事は必読です。善意が必ずしも良い結果をもたらすわけではないことを理解するためにも、ぜひ一度目を通してみてください。","arguments":[{"arg_id":"A4_1","argument":"こうした記事が広まることで、現場での共通意識を作るのが楽になる。","comment_id":"4","x":9.323074,"y":18.559566,"p":0},{"arg_id":"A8_0","argument":"震災に関して何か行動を起こそうとしている人や、考えがある人は、一度これを読んでみると良いと思う。","comment_id":"8","x":9.229061,"y":19.815226,"p":1},{"arg_id":"A9_0","argument":"地震が起きて「何か力になりたい」と思う人は読むべき内容です。すべての善意がいい方向に働くわけではない。","comment_id":"9","x":9.148173,"y":20.54848,"p":0.90384951139003},{"arg_id":"A10_0","argument":"地震が起きて「何か力になりたい」と思う人は読むべき内容です。すべての善意がいい方向に働くわけではない。","comment_id":"10","x":9.799041,"y":19.994802,"p":1}]}],"comments":{"1":{"comment":"\"これは、今から気にしないといけないことかと\""},"2":{"comment":"\"この内容はツィッターで数年前に読んだものだが、今も教訓として色あせない。語り継がれて欲しい。\""},"3":{"comment":"\"防災\""},"4":{"comment":"\"1年前の記事か。でもこういう貴重かつ極めて実務的な経験談を可能な限り多くの人が読んでおけば、いざ現場で共通意識作るのが楽になりそうだなぁ。広まって欲しい。\""},"5":{"comment":"\"“はじめに 東日本大震災。 東日本全体で21000人もの人が命を失い、または行方不明となりました。私も被災し岩手県陸前高田市米崎小学校の体育館で二ヶ月間にわたり避難所生活をしながら避難所運営を経験しました。 多\""},"6":{"comment":"\"2015.06.25\""},"7":{"comment":"\"読んだことを忘れてる。再ブクマ。\""},"8":{"comment":"\"震災の件で、何かしようとしてる人や、何か思う所のある人は、一度これ通して読んでみるといいと思うよ。\""},"9":{"comment":"\"地震が起きて「何か力になりたい」と思う人は読むべき内容です。すべての善意がいい方向に働くわけではない。[東日本大震災][防災][地震] / “東日本大震災、体育館避難所で起きたこと / 佐藤一男 / 防災士 | SYNODOS …”\""},"10":{"comment":"\"地震が起きて「何か力になりたい」と思う人は読むべき内容です。すべての善意がいい方向に働くわけではない。[東日本大震災][防災][地震]\""},"11":{"comment":"\"マニュアルとしても通用する永久保存版だと思う。\""},"12":{"comment":"\"ものすごく参考になる体験記。\""}},"translations":{},"overview":"クラスター0は、過去の情報や経験談が現在でも重要であり、特に実務的な経験談やマニュアルとしての価値が高い情報の保存が必要と強調しています。クラスター1は、東日本大震災の被災者である佐藤一男氏の避難所運営経験が、防災対策の重要性と避難所運営の課題を浮き彫りにしていることを示しています。クラスター2は、震災に対する共通意識を形成し、善意が必ずしも良い結果をもたらさないことを理解するために、この記事が必読であると述べています。","config":{"name":"「東日本大震災、体育館避難所で起きたこと」(SYNODOS)に対する反応","question":"人々は「東日本大震災、体育館避難所で起きたこと」を読んでどのように反応したか","input":"example-gymnasium-shelter2011","model":"gpt-4o","extraction":{"workers":3,"limit":12,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことです。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\n背景は、私の上司が日本の都知事選挙に立候補したということです。\nこれから、出馬に関しての記事に一般から寄せられたコメントの例を挙げます。\nより簡潔で読みやすいものにするのを手伝ってほしい。\n本当に必要な場合は、2つの別々の議論に分けることもできるが、1つの議論を返すのが最善であることが多いだろう。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai \n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n/human \n\nAIの能力、限界、倫理的配慮について一般の人々を教育するための協調的な努力が必要である。\n\n/ai \n\n[\n  \"AIの能力について一般の人々を教育すべきである。\",\n  \"AIの限界と倫理的配慮について、一般の人々を教育すべきである。\"\n]\n\n/human \n\nAIは、エネルギー効率と居住者のウェルビーイングのためにスマートホームやビルを最適化することができる。\n\n/ ai \n\n[\n  \"AIは、エネルギー効率と居住者のウェルビーイングのためにスマートホームやビルを最適化することができる。\"\n]\n\n/human \n\nAIはエネルギー・グリッドを最適化し、無駄と二酸化炭素排出を削減するのに役立つ。\n\n/ai \n\n[\n  \"AIはエネルギー網を最適化し、無駄と二酸化炭素排出を削減することができる。\"\n]\n","model":"gpt-4o"},"clustering":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"},"translation":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, t\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [t(\"Argument\"), t(\"Original comment\"), t(\"Representative arguments\"),\n               t(\"Open full-screen map\"), t(\"Back to report\"), t(\"Hide labels\"), t(\"Show labels\"),\n               t(\"Show filters\"), t(\"Hide filters\"), t(\"Min. votes\"), t(\"Consensus\"),\n               t(\"Showing\"), t(\"arguments\"), t(\"Reset zoom\"), t(\"Click anywhere on the map to close this\"),\n               t(\"Click on the dot for details\"), t(\"agree\"), t(\"disagree\"), t(\"Language\"),\n               t(\"English\"), t(\"arguments\"), t(\"of total\"), t(\"Overview\"), t(\"Cluster analysis\"),\n               t(\"Representative comments\"), t(\"Introduction\"), t(\"Clusters\"), t(\"Appendix\"),\n               t(\"This report was generated using an AI pipeline that consists of the following steps\"),\n               t(\"Step\"), t(\"extraction\"), t(\"show code\"), t(\"hide code\"), t(\"show prompt\"),\n               t(\"hide prompt\"), t(\"embedding\"), t(\"clustering\"), t(\"labelling\"), t(\"takeaways\"),\n               t(\"overview\")]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e","prompt":"/system \n\nあなたはプロの翻訳者です。\n日本語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、{language}に翻訳して返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。"},"intro":"このAIが作成したレポートは、東日本大震災、体育館避難所で起きたこと(https://synodos.jp/opinion/society/14462/)に対するはてなブックマークのコメントのデータに依拠している。","output_dir":"example-gymnasium-shelter2011","embedding":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたは、公開協議の際に参加者の一団から出された議論のリストを渡されます。あなたは、主な論点を1～2段落にまとめて回答します。あなたはとても簡潔で、読みやすい短い文章を書くことができます。\n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門家リサーチアシスタントです。\nあなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。\nあなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。\nあなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":true,"reason":"not trace of previous run"},{"step":"embedding","run":true,"reason":"not trace of previous run"},{"step":"clustering","run":true,"reason":"not trace of previous run"},{"step":"labelling","run":true,"reason":"not trace of previous run"},{"step":"takeaways","run":true,"reason":"not trace of previous run"},{"step":"overview","run":true,"reason":"not trace of previous run"},{"step":"translation","run":true,"reason":"not trace of previous run"},{"step":"aggregation","run":true,"reason":"not trace of previous run"},{"step":"visualization","run":true,"reason":"not trace of previous run"}],"status":"running","start_time":"2024-07-04T17:42:28.177989","completed_jobs":[{"step":"extraction","completed":"2024-07-04T17:42:36.966883","duration":8.787279,"params":{"workers":3,"limit":12,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []\n","prompt":"/system\n\n\nあなたはプロのリサーチ・アシスタントで、私の仕事を手伝うことです。\n私の仕事は、論点を整理したきれいなデータセットを作成することです。\n\n背景は、私の上司が日本の都知事選挙に立候補したということです。\nこれから、出馬に関しての記事に一般から寄せられたコメントの例を挙げます。\nより簡潔で読みやすいものにするのを手伝ってほしい。\n本当に必要な場合は、2つの別々の議論に分けることもできるが、1つの議論を返すのが最善であることが多いだろう。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai \n\n[\n  \"私たちは、AI技術が環境に与える影響の軽減に焦点を当てるべきである\"\n]\n\n/human \n\nAIの能力、限界、倫理的配慮について一般の人々を教育するための協調的な努力が必要である。\n\n/ai \n\n[\n  \"AIの能力について一般の人々を教育すべきである。\",\n  \"AIの限界と倫理的配慮について、一般の人々を教育すべきである。\"\n]\n\n/human \n\nAIは、エネルギー効率と居住者のウェルビーイングのためにスマートホームやビルを最適化することができる。\n\n/ ai \n\n[\n  \"AIは、エネルギー効率と居住者のウェルビーイングのためにスマートホームやビルを最適化することができる。\"\n]\n\n/human \n\nAIはエネルギー・グリッドを最適化し、無駄と二酸化炭素排出を削減するのに役立つ。\n\n/ai \n\n[\n  \"AIはエネルギー網を最適化し、無駄と二酸化炭素排出を削減することができる。\"\n]\n","model":"gpt-4o"}},{"step":"embedding","completed":"2024-07-04T17:42:37.855435","duration":0.887537,"params":{"source_code":"\nfrom langchain.embeddings import OpenAIEmbeddings\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeds = OpenAIEmbeddings().embed_documents(args)\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}},{"step":"clustering","completed":"2024-07-04T17:42:47.856621","duration":9.999886,"params":{"clusters":3,"source_code":"\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom importlib import import_module\n\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # (!) we import the following modules dynamically for a reason\n    # (they are slow to load and not required for all pipelines)\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    stopwords = import_module('nltk.corpus').stopwords\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module(\n        'sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    stop = stopwords.words(\"english\")\n    vectorizer_model = CountVectorizer(stop_words=stop)\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # Fit the topic model.\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,  # Use the modified n_neighbors\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result\n"}},{"step":"labelling","completed":"2024-07-04T17:42:49.712172","duration":1.854674,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nあなたは、より広範なコンサルテーション内の一連の議論に対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。あなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルは非常に簡潔でなければならず、クラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o"}},{"step":"takeaways","completed":"2024-07-04T17:42:56.165486","duration":6.452293,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm(messages=messages(prompt, input)).content.strip()\n    return response\n","prompt":"/system \n\nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたは、公開協議の際に参加者の一団から出された議論のリストを渡されます。あなたは、主な論点を1～2段落にまとめて回答します。あなたはとても簡潔で、読みやすい短い文章を書くことができます。\n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o"}},{"step":"overview","completed":"2024-07-04T17:43:00.150871","duration":3.984924,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)\n","prompt":"/system \n\nあなたはシンクタンクで働く専門家リサーチアシスタントです。\nあなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。\nあなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。\nあなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o"}},{"step":"translation","completed":"2024-07-04T17:43:00.154809","duration":0.002102,"params":{"model":"gpt-4","languages":[],"flags":[],"source_code":"import json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain.chat_models import ChatOpenAI\nfrom utils import messages, t\nfrom langchain.schema import AIMessage\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\n\ndef translation(config):\n\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/translations.json\"\n    results = {}\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) == 0:\n        print(\"No languages specified. Skipping translation step.\")\n        # creating an empty file any, to reduce special casing later\n        with open(path, 'w') as file:\n            json.dump(results, file, indent=2)\n        return\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    with open(f\"outputs/{dataset}/overview.txt\") as f:\n        overview = f.read()\n\n    UI_copy = [t(\"Argument\"), t(\"Original comment\"), t(\"Representative arguments\"),\n               t(\"Open full-screen map\"), t(\"Back to report\"), t(\"Hide labels\"), t(\"Show labels\"),\n               t(\"Show filters\"), t(\"Hide filters\"), t(\"Min. votes\"), t(\"Consensus\"),\n               t(\"Showing\"), t(\"arguments\"), t(\"Reset zoom\"), t(\"Click anywhere on the map to close this\"),\n               t(\"Click on the dot for details\"), t(\"agree\"), t(\"disagree\"), t(\"Language\"),\n               t(\"English\"), t(\"arguments\"), t(\"of total\"), t(\"Overview\"), t(\"Cluster analysis\"),\n               t(\"Representative comments\"), t(\"Introduction\"), t(\"Clusters\"), t(\"Appendix\"),\n               t(\"This report was generated using an AI pipeline that consists of the following steps\"),\n               t(\"Step\"), t(\"extraction\"), t(\"show code\"), t(\"hide code\"), t(\"show prompt\"),\n               t(\"hide prompt\"), t(\"embedding\"), t(\"clustering\"), t(\"labelling\"), t(\"takeaways\"),\n               t(\"overview\")]\n\n    arg_list = arguments['argument'].to_list() + \\\n        labels['label'].to_list() + \\\n        UI_copy + \\\n        languages\n\n    if 'name' in config:\n        arg_list.append(config['name'])\n    if 'question' in config:\n        arg_list.append(config['question'])\n\n    prompt_file = config.get('translation_prompt', 'default')\n    with open(f\"prompts/translation/{prompt_file}.txt\") as f:\n        prompt = f.read()\n    model = config['model']\n\n    config['translation_prompt'] = prompt\n\n    translations = [translate_lang(\n        arg_list, 10, prompt, lang, model) for lang in languages]\n\n    # handling long takeaways differently, WITHOUT batching too much\n    long_arg_list = takeaways['takeaways'].to_list()\n    long_arg_list.append(overview)\n    if 'intro' in config:\n        long_arg_list.append(config['intro'])\n\n    long_translations = [translate_lang(\n        long_arg_list, 1, prompt, lang, model) for lang in languages]\n\n    for i, id in enumerate(arg_list):\n        print('i, id', i, id)\n        results[str(id)] = list([t[i] for t in translations])\n    for i, id in enumerate(long_arg_list):\n        results[str(id)] = list([t[i] for t in long_translations])\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n\n\ndef translate_lang(arg_list, batch_size, prompt, lang, model):\n    translations = []\n    lang_prompt = prompt.replace(\"{language}\", lang)\n    print(f\"Translating to {lang}...\")\n    for i in tqdm(range(0, len(arg_list), batch_size)):\n        batch = arg_list[i: i + batch_size]\n        translations.extend(translate_batch(batch, lang_prompt, model))\n    return translations\n\n\ndef translate_batch(batch, lang_prompt, model, retries=3):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = json.dumps(list(batch))\n    response = llm(messages=messages(lang_prompt, input)).content.strip()\n    if \"```\" in response:\n        response = response.split(\"```\")[1]\n    if response.startswith(\"json\"):\n        response = response[4:]\n    try:\n        parsed = [a.strip() for a in json.loads(response)]\n        if len(parsed) != len(batch):\n            print(\"Warning: batch size mismatch!\")\n            print(\"Batch len:\", len(batch))\n            print(\"Response len:\", len(parsed))\n            for i, item in enumerate(batch):\n                print(f\"Batch item {i}:\", item)\n                if (i \u003c len(parsed)):\n                    print(\"Response:\", parsed[i])\n            if (len(batch) \u003e 1):\n                print(\"Retrying with smaller batches...\")\n                mid = len(batch) // 2\n                return translate_batch(batch[:mid], lang_prompt, model, retries - 1) + \\\n                    translate_batch(\n                        batch[mid:], lang_prompt, model, retries - 1)\n            else:\n                print(\"Retrying batch...\")\n                return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            return parsed\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying batch...\")\n            return translate_batch(batch, lang_prompt, model, retries - 1)\n        else:\n            raise e","prompt":"/system \n\nあなたはプロの翻訳者です。\n日本語で書かれた単語と文章のリストを受け取ります。\n同じリストを同じ順番で、{language}に翻訳して返してください。\n元のリストと同じ長さの文字列の有効なJSONリストを返すようにしてください。"}}],"lock_until":"2024-07-04T17:48:00.156431","current_job":"aggregation","current_job_started":"2024-07-04T17:43:00.156420"}}},"__N_SSG":true},"page":"/","query":{},"buildId":"JWsllBIcgSGhKLoictODg","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>